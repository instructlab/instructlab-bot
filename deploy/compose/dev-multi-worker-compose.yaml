# Multi-Worker Compose File
# Each worker needs its own volume to mount with a clone of the taxonomy repo, example uses /home/fedora/instructlab-worker(n).
# Example local generation:
# Once the worker starts, exec in and start 'ilab serve'
# Next, start instruct-lab-bot-worker with the following:
# export AWS_ACCESS_KEY_ID=x
# export AWS_SECRET_ACCESS_KEY=x
# export AWS_DEFAULT_REGION=x
# git config --global --add safe.directory /data/taxonomy
# instruct-lab-bot-worker generate --github-token <YOUR_GITHUB_TOKEN_FOR_THE_TAXONOMY_REPO> \
# --redis redis:6379  \
# --s3-bucket some-bucket-name \
# --aws-region us-east-1
services:
  redis:
    container_name: redis
    image: redis:latest
    ports:
      - 6379:6379

  bot:
    container_name: bot
    image: ghcr.io/instruct-lab/instruct-lab-bot/instruct-lab-gobot:main
    env_file:
      - .env
    depends_on:
      - redis

  worker1:
    container_name: worker1
    image: ghcr.io/instruct-lab/instruct-lab-bot/instruct-lab-serve:main
    volumes:
      - /home/fedora/instructlab-worker1:/data
    entrypoint: ["/bin/bash"]
    stdin_open: true
    tty: true
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  worker2:
    container_name: worker2
    image: ghcr.io/instruct-lab/instruct-lab-bot/instruct-lab-serve:main
    volumes:
      - /home/fedora/instructlab-worker2:/data
    entrypoint: ["/bin/bash"]
    stdin_open: true
    tty: true
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  worker3:
    container_name: worker3
    image: ghcr.io/instruct-lab/instruct-lab-bot/instruct-lab-serve:main
    volumes:
      - /home/fedora/instructlab-worker3:/data
    entrypoint: ["/bin/bash"]
    stdin_open: true
    tty: true
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

#  Workflow between services above
#  1. PR is opened by the Contributor.
#  2. User runs `@instruct-lab-bot <function>` in a comment on the PR. Current job types are `[generate-local, precheck, generate, enable]`
#  3. Bot checks for appropriate labels and functions
#  4. Bot pushes the job to a Redis queue.
#  5. Worker(s) pop the job sequentially off the queue and process the requested job from the PR.
#  6. Worker checks out the PR for the job.
#  7. Worker runs the requested job type:
#  - `@instruct-lab-bot generate-local` runs generate against the local model running on the worker node
#  - `@instruct-lab-bot precheck` runs chat against the FMaaS backend infra.
#  - `@instruct-lab-bot generate` makes a direct api call to the SDG backend for processing for each taxonomy file modified/added.
#  - `@instruct-lab-bot enable` if enabled on a PR, the user can then exercise bot commands on the shared infrastructure.
#  8. Worker pushes the results to the S3 bucket and generates a public URL.
#  9. Worker pushes metadata back on to the job ID in the Redis queue.
#  10. Bot processes the results from the worker and replies to the PR: "Here is the generated data ..."
